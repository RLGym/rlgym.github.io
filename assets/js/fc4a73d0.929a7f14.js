"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[501],{4137:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>f});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},m=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),u=c(n),d=i,f=u["".concat(l,".").concat(d)]||u[d]||p[d]||o;return n?r.createElement(f,a(a({ref:t},m),{},{components:n})):r.createElement(f,a({ref:t},m))}));function f(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,a=new Array(o);a[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:i,a[1]=s;for(var c=2;c<o;c++)a[c]=n[c];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},7962:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var r=n(7462),i=(n(7294),n(4137));const o={title:"Overview",sidebar_position:2},a="Overview of RLGym",s={unversionedId:"Getting Started/overview",id:"Getting Started/overview",title:"Overview",description:'RLGym is designed to make creating and modifying reinforcement learning environments as simple as possible. The RLGym API breaks down an environment into several components, which we refer to as "configuration objects". All a user needs to do to create their own environment is write a concrete implementation of the configuration objects they want to use, and RLGym will handle the rest. Our goal is to provide a clear path from asking the question "Is it possible to use reinforcement learning with this game?" to actually training an agent.',source:"@site/docs/Getting Started/overview.md",sourceDirName:"Getting Started",slug:"/Getting Started/overview",permalink:"/Getting Started/overview",draft:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Overview",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/Getting Started/introduction"},next:{title:"Quick Start Guide",permalink:"/Getting Started/quickstart"}},l={},c=[{value:"Configuration Objects",id:"configuration-objects",level:2}],m={toc:c},u="wrapper";function p(e){let{components:t,...n}=e;return(0,i.kt)(u,(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"overview-of-rlgym"},"Overview of RLGym"),(0,i.kt)("p",null,'RLGym is designed to make creating and modifying reinforcement learning environments as simple as possible. The RLGym API breaks down an environment into several components, which we refer to as "configuration objects". All a user needs to do to create their own environment is write a concrete implementation of the configuration objects they want to use, and RLGym will handle the rest. Our goal is to provide a clear path from asking the question "Is it possible to use reinforcement learning with this game?" to actually training an agent.'),(0,i.kt)("h2",{id:"configuration-objects"},"Configuration Objects"),(0,i.kt)("p",null,'Every RLGym environment is built from the following components, which we refer to as "configuration objects":'),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A ",(0,i.kt)("inlineCode",{parentName:"li"},"TransitionEngine"),": Manages state transitions and core environment logic"),(0,i.kt)("li",{parentName:"ul"},"A ",(0,i.kt)("inlineCode",{parentName:"li"},"StateMutator"),": Controls how environment state is modified (e.g., on reset)"),(0,i.kt)("li",{parentName:"ul"},"An ",(0,i.kt)("inlineCode",{parentName:"li"},"ObsBuilder"),": Converts environment state into agent observations"),(0,i.kt)("li",{parentName:"ul"},"An ",(0,i.kt)("inlineCode",{parentName:"li"},"ActionParser"),": Defines and validates agent actions"),(0,i.kt)("li",{parentName:"ul"},"A ",(0,i.kt)("inlineCode",{parentName:"li"},"RewardFunction"),": Calculates rewards for agent actions"),(0,i.kt)("li",{parentName:"ul"},"One or more ",(0,i.kt)("inlineCode",{parentName:"li"},"DoneConditions"),": Determine when episodes end (termination) or are cut short (truncation)"),(0,i.kt)("li",{parentName:"ul"},"Optionally a ",(0,i.kt)("inlineCode",{parentName:"li"},"Renderer"),": Visualizes the environment state")),(0,i.kt)("p",null,"The diagram below depicts how RLGym configuration objects come together to define an environment and interface with\na learning agent."),(0,i.kt)("mermaid",{value:"%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'primaryColor': '#6f48c0',\n      'primaryTextColor': '#fff',\n      'primaryBorderColor': '#5d2c84',\n      'lineColor': '#808080',\n      'secondaryColor': '#8d8d8d',\n      'tertiaryColor': '#fff'\n    },\n    'flowchart': { 'curve': 'bump' }\n  }\n}%%\nflowchart TB\n    A[StateMutators] --\x3e|Initial State| B[Transition Engine]\n    B --\x3e|Game State| C[Renderer]\n    B --\x3e D{{Agents, Next State, Shared Info}}\n    D --\x3e E[ObsBuilder]\n    D --\x3e F[RewardFunction]\n    D --\x3e G[TerminalConditions]\n    D --\x3e H[TruncationConditions]\n    E --\x3e|Observation| I[Learning Agent] \n    F --\x3e|Reward| I\n    G --\x3e|Terminated| I\n    H --\x3e|Truncated| I\n    I --\x3e|Action| J[ActionParser]\n    J --\x3e|Engine Controls| B"}),(0,i.kt)("p",null,"To see an example of a concrete implementation of these configuration objects, see our ",(0,i.kt)("a",{parentName:"p",href:"../../Custom%20Environments/custom-environment"},"Custom Environment")," guide, or check out the individual examples relevant to Rocket League in our ",(0,i.kt)("a",{parentName:"p",href:"../../Rocket%20League/Configuration%20Objects/action_parsers"},"Rocket League")," section."))}p.isMDXComponent=!0}}]);