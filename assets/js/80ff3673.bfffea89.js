"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[381],{4137:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>m});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=r.createContext({}),l=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=l(n),f=a,m=p["".concat(c,".").concat(f)]||p[f]||d[f]||o;return n?r.createElement(m,i(i({ref:t},u),{},{components:n})):r.createElement(m,i({ref:t},u))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=f;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[p]="string"==typeof e?e:a,i[1]=s;for(var l=2;l<o;l++)i[l]=n[l];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}f.displayName="MDXCreateElement"},1909:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var r=n(7462),a=(n(7294),n(4137));const o={title:"Reward Functions"},i="Reward Functions",s={unversionedId:"Rocket League/Configuration Objects/reward_functions",id:"Rocket League/Configuration Objects/reward_functions",title:"Reward Functions",description:"A RewardFunction is an object used by an RLGym environment to compute the rewards for each agent every step.",source:"@site/docs/Rocket League/Configuration Objects/reward_functions.md",sourceDirName:"Rocket League/Configuration Objects",slug:"/Rocket League/Configuration Objects/reward_functions",permalink:"/Rocket League/Configuration Objects/reward_functions",draft:!1,tags:[],version:"current",frontMatter:{title:"Reward Functions"},sidebar:"tutorialSidebar",previous:{title:"Renderers",permalink:"/Rocket League/Configuration Objects/renderers"},next:{title:"State Mutators",permalink:"/Rocket League/Configuration Objects/state_mutators"}},c={},l=[{value:"Example",id:"example",level:2}],u={toc:l},p="wrapper";function d(e){let{components:t,...n}=e;return(0,a.kt)(p,(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"reward-functions"},"Reward Functions"),(0,a.kt)("p",null,"A ",(0,a.kt)("inlineCode",{parentName:"p"},"RewardFunction")," is an object used by an RLGym environment to compute the rewards for each agent every step."),(0,a.kt)("p",null,"All reward functions must implement the following methods:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# Called every time `TransitionEngine.create_base_state()` is called.\ndef reset(self, agents: List[AgentID], initial_state: StateType, shared_info: Dict[str, Any]) -> None:\n\n# Called every time `TransitionEngine.step()` is called.\ndef get_rewards(self, agents: List[AgentID], state: StateType, is_terminated: Dict[AgentID, bool], \n               is_truncated: Dict[AgentID, bool], shared_info: Dict[str, Any]) -> Dict[AgentID, RewardType]:\n")),(0,a.kt)("h2",{id:"example"},"Example"),(0,a.kt)("p",null,"Here's a simple reward function that encourages agents to drive faster - the faster they go, the more reward they get:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from typing import List, Dict, Any, Union, Tuple\n\nfrom rlgym.api import RewardFunction, AgentID\nfrom rlgym.rocket_league.api import GameState\nimport numpy as np\n\n\nclass SpeedReward(RewardFunction):\n  def reset(self, agents: List[AgentID], initial_state: GameState, shared_info: Dict[str, Any]) -> None:\n      pass\n  \n  def get_rewards(self, agents: List[AgentID], state: GameState, is_terminated: Dict[AgentID, bool],\n                  is_truncated: Dict[AgentID, bool], shared_info: Dict[str, Any]) -> Dict[AgentID, float]:\n      \n    rewards = {}\n    for agent in agents:\n        car = state.cars[agent]\n        linear_velocity = car.physics.linear_velocity\n        reward = np.linalg.norm(linear_velocity)  # Reward is the car's speed\n        rewards[agent] = reward\n        \n    return rewards\n")),(0,a.kt)("p",null,"That's it! Just pass this reward function to your environment and watch your agents learn to zoom around the field."))}d.isMDXComponent=!0}}]);