"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[359],{4137:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),d=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=d(e.components);return r.createElement(l.Provider,{value:t},e.children)},p="mdxType",f={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(n),u=a,m=p["".concat(l,".").concat(u)]||p[u]||f[u]||i;return n?r.createElement(m,o(o({ref:t},c),{},{components:n})):r.createElement(m,o({ref:t},c))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:a,o[1]=s;for(var d=2;d<i;d++)o[d]=n[d];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},1355:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>f,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var r=n(7462),a=(n(7294),n(4137));const i={title:"Creating an Environment",sidebar_position:2},o="Creating an Environment",s={unversionedId:"Custom Environments/custom-environment",id:"Custom Environments/custom-environment",title:"Creating an Environment",description:"This tutorial will show you how to create a simple grid world environment using the RLGym API. Every RLGym environment must implement the configuration objects specified in our RLGym overview. Let's take a look at the following example to see how each of the configuration objects can be implemented and used.",source:"@site/docs/Custom Environments/custom-environment.md",sourceDirName:"Custom Environments",slug:"/Custom Environments/custom-environment",permalink:"/Custom Environments/custom-environment",draft:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Creating an Environment",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"State Mutators",permalink:"/Rocket League/Configuration Objects/state_mutators"},next:{title:"List of Game Values",permalink:"/Cheatsheets/game_values"}},l={},d=[{value:"Grid World Example",id:"grid-world-example",level:2}],c={toc:d},p="wrapper";function f(e){let{components:t,...n}=e;return(0,a.kt)(p,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"creating-an-environment"},"Creating an Environment"),(0,a.kt)("p",null,"This tutorial will show you how to create a simple grid world environment using the RLGym API. Every RLGym environment must implement the configuration objects specified in our ",(0,a.kt)("a",{parentName:"p",href:"/Getting%20Started/overview"},"RLGym overview"),". Let's take a look at the following example to see how each of the configuration objects can be implemented and used."),(0,a.kt)("h2",{id:"grid-world-example"},"Grid World Example"),(0,a.kt)("p",null,"We'll start with the transition engine, which defines the states environment dynamics."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from typing import Dict, List, Tuple, Optional\nimport numpy as np\nfrom dataclasses import dataclass\nfrom rlgym.api import TransitionEngine, StateMutator, ObsBuilder, ActionParser, RewardFunction, DoneCondition\n\n# First, define our state type\n@dataclass\nclass GridWorldState:\n    agent_pos: np.ndarray  # [x, y]\n    target_pos: np.ndarray  # [x, y]\n    obstacles: List[np.ndarray]  # List of [x, y] positions\n    grid_size: int\n    steps: int = 0\n\n# Now we implement our Transition Engine, which is the core of the environment.\nclass GridWorldEngine(TransitionEngine[int, GridWorldState, int]):\n    """Handles the core game logic"""\n    def __init__(self, grid_size: int):\n        self.grid_size = grid_size\n        self._state = None\n        self._config = {}\n        \n    @property\n    def agents(self) -> List[int]:\n        return [0]  # Single agent environment\n        \n    @property\n    def max_num_agents(self) -> int:\n        return 1  # This environment only supports one agent\n        \n    @property\n    def state(self) -> GridWorldState:\n        return self._state\n        \n    @property\n    def config(self) -> Dict[str, Any]:\n        return self._config\n        \n    @config.setter\n    def config(self, value: Dict[str, Any]):\n        self._config = value\n        \n    def step(self, actions: Dict[int, int], shared_info: Dict[str, Any]) -> GridWorldState:\n        action = actions[0]  # Get action for our single agent\n        current_pos = self._state.agent_pos.copy()\n        \n        # Apply movement: 0=up, 1=right, 2=down, 3=left\n        if action == 0:   current_pos[1] += 1\n        elif action == 1: current_pos[0] += 1\n        elif action == 2: current_pos[1] -= 1\n        elif action == 3: current_pos[0] -= 1\n        \n        # Ensure we stay in bounds\n        current_pos = np.clip(current_pos, 0, self.grid_size - 1)\n        \n        # Check if move is valid (not into obstacle)\n        if not any(np.array_equal(current_pos, obs) for obs in self._state.obstacles):\n            self._state.agent_pos = current_pos\n            \n        self._state.steps += 1\n        return self._state\n        \n    def create_base_state(self) -> GridWorldState:\n        # Create a minimal state for the mutator to modify\n        return GridWorldState(\n            agent_pos=np.zeros(2),  # Will be set by mutator\n            target_pos=np.zeros(2),  # Will be set by mutator\n            obstacles=[],  # Will be set by mutator\n            grid_size=self.grid_size,\n            steps=0\n        )\n        \n    def reset(self, initial_state: Optional[GridWorldState] = None) -> None:\n        """Reset the engine with an optional initial state"""\n        self._state = initial_state if initial_state is not None else self.create_base_state()\n')),(0,a.kt)("p",null,"Now that the hard part is out of the way, we can implement the other configuration objects that will shape what agents see, the actions they can take, and how they should be rewarded."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# We need to define a state mutator, which is responsible for modifying the environment state.\nclass GridWorldMutator(StateMutator[GridWorldState]):\n    """Controls environment reset and state modifications"""\n    def __init__(self, grid_size: int, num_obstacles: int = 3):\n        self.grid_size = grid_size\n        self.num_obstacles = num_obstacles\n        \n    def apply(self, state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        # Random agent and target positions\n        state.agent_pos = np.random.randint(0, self.grid_size, size=2)\n        state.target_pos = np.random.randint(0, self.grid_size, size=2)\n        \n        # Random obstacle positions (ensuring they don\'t overlap)\n        state.obstacles = []\n        while len(state.obstacles) < self.num_obstacles:\n            obs = np.random.randint(0, self.grid_size, size=2)\n            if not (np.array_equal(obs, state.agent_pos) or \n                   np.array_equal(obs, state.target_pos) or \n                   any(np.array_equal(obs, o) for o in state.obstacles)):\n                state.obstacles.append(obs)\n\n# Here is the Observation Builder, which will convert the environment state into agent observations.\nclass GridWorldObs(ObsBuilder[int, np.ndarray, GridWorldState, np.ndarray]):\n    """Converts state into agent observations"""\n        \n    def get_obs_space(self, agent: int) -> np.ndarray:\n        # [agent_x, agent_y, target_x, target_y, obstacles]\n        return np.zeros(4 + 2*3, dtype=np.float32)  # Assuming max 3 obstacles\n        \n    def reset(self, agents: List[int], initial_state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        pass\n        \n    def build_obs(self, agents: List[int], state: GridWorldState, shared_info: Dict[str, Any]) -> Dict[int, np.ndarray]:\n        # Build observation for each agent\n        observations = {}\n        for agent in agents:\n            # [agent_x, agent_y, target_x, target_y, obstacle positions]\n            obs = np.concatenate([\n                state.agent_pos,\n                state.target_pos,\n                np.concatenate(state.obstacles)\n            ])\n            observations[agent] = obs\n        return observations\n\n# Next we need an Action Parser, which will define what actions agents can take. \n# Because we are using a simple discrete action space, we don\'t need to do anything special here, \n# so we\'ll just pass the actions from the agent straight through to the transition engine.\nclass GridWorldActions(ActionParser[int, int, int, GridWorldState, int]):\n    """Defines the action space and parsing"""\n    def get_action_space(self, agent: int) -> int:\n        return 4  # Up, Right, Down, Left\n        \n    def reset(self, agents: List[int], initial_state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        pass  # No state to reset\n        \n    def parse_actions(self, actions: Dict[int, int], state: GridWorldState, shared_info: Dict[str, Any]) -> Dict[int, int]:\n        # Actions are already in the correct format\n        return actions\n\n# Now we need a Reward Function, which will calculate rewards for the agents.\nclass GridWorldReward(RewardFunction[int, GridWorldState, float]):\n    """Calculates rewards for actions"""\n    def __init__(self, goal_reward: float = 10.0, step_penalty: float = -0.1):\n        self.goal_reward = goal_reward\n        self.step_penalty = step_penalty\n        \n    def reset(self, agents: List[int], initial_state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        pass  # No state to reset\n        \n    def get_rewards(self, agents: List[int], state: GridWorldState, \n                   is_terminated: Dict[int, bool], is_truncated: Dict[int, bool],\n                   shared_info: Dict[str, Any]) -> Dict[int, float]:\n        rewards = {}\n        for agent in agents:\n            if np.array_equal(state.agent_pos, state.target_pos):\n                # If we reached the target, provide the goal reward.\n                rewards[agent] = self.goal_reward\n            else:\n                # If we haven\'t reached the target, apply a step penalty.\n                rewards[agent] = self.step_penalty\n        return rewards\n\n# Finally we\'ll create Terminal and Truncated conditions, which will determine when episodes end naturally.\nclass GridWorldTerminalCondition(DoneCondition[int, GridWorldState]):\n    """Determines when episodes naturally end (reaching the goal)"""\n    def reset(self, agents: List[int], initial_state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        pass\n        \n    def is_done(self, agent: int, state: GridWorldState) -> bool:\n        # Episode ends naturally when we reach the target\n        return np.array_equal(state.agent_pos, state.target_pos)\n\nclass GridWorldTruncatedCondition(DoneCondition[int, GridWorldState]):\n    """Determines when episodes are cut short (timeout)"""\n    def __init__(self, max_steps: int = 100):\n        self.max_steps = max_steps\n        \n    def reset(self, agents: List[int], initial_state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        pass\n        \n    def is_done(self, agent: int, state: GridWorldState) -> bool:\n        # Episode is truncated if we exceed max steps\n        return state.steps >= self.max_steps\n\n')),(0,a.kt)("p",null,"And that's it for our configuration objects! To use our new gridworld environment, we just pass the configuration objects to the RLGym constructor."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# Build the environment\nenv = RLGym(\n        state_mutator=GridWorldMutator(grid_size),\n        obs_builder=GridWorldObs(),\n        action_parser=GridWorldActions(),\n        reward_fn=GridWorldReward(),\n        transition_engine=GridWorldEngine(grid_size),\n        termination_cond=GridWorldTerminalCondition(),\n        truncation_cond=GridWorldTruncatedCondition(),\n    )\n\n# Interact with our gridworld like any other RLGym environment.\nobs = env.reset()\nep_rew = 0\n\nfor _ in range(1000):\n    action = {0: env.action_space.sample()}  # Random action\n    obs, reward, done, truncated, info = env.step(action)\n    ep_rew += reward[0] # Reward for agent 0\n    \n    if done or truncated:\n        obs = env.reset()\n        print(f"Episode reward: {ep_rew}")\n        ep_rew = 0\n')),(0,a.kt)("p",null,"Now we are ready to plug this into a learning algorithm and train a gridworld agent!"))}f.isMDXComponent=!0}}]);