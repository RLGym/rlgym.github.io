"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[941],{4137:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>g});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=c(n),d=r,g=m["".concat(l,".").concat(d)]||m[d]||u[d]||i;return n?a.createElement(g,o(o({ref:t},p),{},{components:n})):a.createElement(g,o({ref:t},p))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[m]="string"==typeof e?e:r,o[1]=s;for(var c=2;c<i;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},9285:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var a=n(7462),r=(n(7294),n(4137));const i={title:"Training an Agent",sidebar_position:1},o="Training an Agent",s={unversionedId:"Rocket League/training_an_agent",id:"Rocket League/training_an_agent",title:"Training an Agent",description:"This guide will walk you through training a Rocket League bot using RLGym and the PPOLearner from RLGym Learn. We'll use RocketSim to avoid running the actual game, and cover the essential concepts you need to understand. We'll be jumping off from the end of our Quick Start Guide, so make sure you've read that first.",source:"@site/docs/Rocket League/training_an_agent.md",sourceDirName:"Rocket League",slug:"/Rocket League/training_an_agent",permalink:"/Rocket League/training_an_agent",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Training an Agent",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Quick Start Guide",permalink:"/Getting Started/quickstart"},next:{title:"Action Parsers",permalink:"/Rocket League/Configuration Objects/action_parsers"}},l={},c=[{value:"A Better Agent",id:"a-better-agent",level:2},{value:"Understanding the Training Process",id:"understanding-the-training-process",level:2},{value:"Monitoring Progress",id:"monitoring-progress",level:2}],p={toc:c},m="wrapper";function u(e){let{components:t,...n}=e;return(0,r.kt)(m,(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"training-an-agent"},"Training an Agent"),(0,r.kt)("p",null,"This guide will walk you through training a Rocket League bot using RLGym and the PPOLearner from RLGym Learn. We'll use RocketSim to avoid running the actual game, and cover the essential concepts you need to understand. We'll be jumping off from the end of our ",(0,r.kt)("a",{parentName:"p",href:"../Getting%20Started/quickstart"},"Quick Start Guide"),", so make sure you've read that first."),(0,r.kt)("h2",{id:"a-better-agent"},"A Better Agent"),(0,r.kt)("p",null,"This tutorial is taken from the wonderful tutorial written by Zealan, the creator of RocketSim, and adapted for RLGym v2. Please check out the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ZealanL/RLGym-PPO-Guide/tree/main"},"original tutorial")," for more details."),(0,r.kt)("p",null,"Now that we've covered the basics, lets dive into a more complicated agent training setup. First, we'll create a better environment where the agent receives a richer reward signal. Next, we'll set all of the hyperparameters in the learner to some reasonable values instead of the defaults. Finally, we'll configure the environment to use a visualizer so we can watch the agent learn as it plays."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from typing import List, Dict, Any\nfrom rlgym.api import RewardFunction\nfrom rlgym.rocket_league.api import GameState, AgentID\nfrom rlgym.rocket_league import common_values\nimport numpy as np\n\nclass SpeedTowardBallReward(RewardFunction[AgentID, GameState, float]):\n    """Rewards the agent for moving quickly toward the ball"""\n    \n    def reset(self, agents: List[AgentID], initial_state: GameState, shared_info: Dict[str, Any]) -> None:\n        pass\n    \n    def get_rewards(self, agents: List[AgentID], state: GameState, is_terminated: Dict[AgentID, bool],\n                    is_truncated: Dict[AgentID, bool], shared_info: Dict[str, Any]) -> Dict[AgentID, float]:\n        rewards = {}\n        for agent in agents:\n            car = state.cars[agent]\n            car_physics = car.physics if car.is_orange else car.inverted_physics\n            ball_physics = state.ball if car.is_orange else state.inverted_ball\n            player_vel = car_physics.linear_velocity\n            pos_diff = (ball_physics.position - car_physics.position)\n            dist_to_ball = np.linalg.norm(pos_diff)\n            dir_to_ball = pos_diff / dist_to_ball\n\n            speed_toward_ball = np.dot(player_vel, dir_to_ball)\n\n            rewards[agent] = max(speed_toward_ball / common_values.CAR_MAX_SPEED, 0.0)\n        return rewards\n\nclass InAirReward(RewardFunction[AgentID, GameState, float]):\n    """Rewards the agent for being in the air"""\n    \n    def reset(self, agents: List[AgentID], initial_state: GameState, shared_info: Dict[str, Any]) -> None:\n        pass\n    \n    def get_rewards(self, agents: List[AgentID], state: GameState, is_terminated: Dict[AgentID, bool],\n                    is_truncated: Dict[AgentID, bool], shared_info: Dict[str, Any]) -> Dict[AgentID, float]:\n        return {agent: float(not state.cars[agent].on_ground) for agent in agents}\n\nclass VelocityBallToGoalReward(RewardFunction[AgentID, GameState, float]):\n    """Rewards the agent for hitting the ball toward the opponent\'s goal"""\n    \n    def reset(self, agents: List[AgentID], initial_state: GameState, shared_info: Dict[str, Any]) -> None:\n        pass\n    \n    def get_rewards(self, agents: List[AgentID], state: GameState, is_terminated: Dict[AgentID, bool],\n                    is_truncated: Dict[AgentID, bool], shared_info: Dict[str, Any]) -> Dict[AgentID, float]:\n        rewards = {}\n        for agent in agents:\n            car = state.cars[agent]\n            if car.is_orange:\n                ball = state.ball\n                goal_y = common_values.BACK_NET_Y\n            else:\n                ball = state.inverted_ball\n                goal_y = -common_values.BACK_NET_Y\n\n            ball_vel = ball.linear_velocity\n            pos_diff = np.array([0, goal_y, 0]) - ball.position\n            dist = np.linalg.norm(pos_diff)\n            dir_to_goal = pos_diff / dist\n            \n            vel_toward_goal = np.dot(ball_vel, dir_to_goal)\n            rewards[agent] = max(vel_toward_goal / common_values.BALL_MAX_SPEED, 0)\n        return rewards\n\ndef build_rlgym_v2_env():\n    from rlgym.api import RLGym\n    from rlgym.rocket_league.action_parsers import LookupTableAction, RepeatAction\n    from rlgym.rocket_league.done_conditions import GoalCondition, NoTouchTimeoutCondition, TimeoutCondition, AnyCondition\n    from rlgym.rocket_league.obs_builders import DefaultObs\n    from rlgym.rocket_league.reward_functions import CombinedReward, GoalReward\n    from rlgym.rocket_league.sim import RocketSimEngine\n    from rlgym.rocket_league.state_mutators import MutatorSequence, FixedTeamSizeMutator, KickoffMutator\n    from rlgym.rocket_league import common_values\n    from rlgym_ppo.util import RLGymV2GymWrapper\n\n    spawn_opponents = True\n    team_size = 1\n    blue_team_size = team_size\n    orange_team_size = team_size if spawn_opponents else 0\n    action_repeat = 8\n    no_touch_timeout_seconds = 30\n    game_timeout_seconds = 300\n\n    action_parser = RepeatAction(LookupTableAction(), repeats=action_repeat)\n    termination_condition = GoalCondition()\n    truncation_condition = AnyCondition(\n        NoTouchTimeoutCondition(timeout_seconds=no_touch_timeout_seconds),\n        TimeoutCondition(timeout_seconds=game_timeout_seconds)\n    )\n\n    reward_fn = CombinedReward(\n        (InAirReward(), 0.002),\n        (SpeedTowardBallReward(), 0.01),\n        (VelocityBallToGoalReward(), 0.1),\n        (GoalReward(), 10.0)\n    )\n\n    obs_builder = DefaultObs(zero_padding=None,\n                           pos_coef=np.asarray([1 / common_values.SIDE_WALL_X, \n                                              1 / common_values.BACK_NET_Y, \n                                              1 / common_values.CEILING_Z]),\n                           ang_coef=1 / np.pi,\n                           lin_vel_coef=1 / common_values.CAR_MAX_SPEED,\n                           ang_vel_coef=1 / common_values.CAR_MAX_ANG_VEL,\n                           boost_coef=1 / 100.0)\n\n    state_mutator = MutatorSequence(\n        FixedTeamSizeMutator(blue_size=blue_team_size, orange_size=orange_team_size),\n        KickoffMutator()\n    )\n\n    rlgym_env = RLGym(\n        state_mutator=state_mutator,\n        obs_builder=obs_builder,\n        action_parser=action_parser,\n        reward_fn=reward_fn,\n        termination_cond=termination_condition,\n        truncation_cond=truncation_condition,\n        transition_engine=RocketSimEngine()\n    )\n\n    return RLGymV2GymWrapper(rlgym_env)\n\n\nif __name__ == "__main__":\n    from rlgym_ppo import Learner\n\n    # 32 processes\n    n_proc = 32\n\n    # educated guess - could be slightly higher or lower\n    min_inference_size = max(1, int(round(n_proc * 0.9)))\n\n    learner = Learner(build_rlgym_v2_env,\n                      n_proc=n_proc,\n                      min_inference_size=min_inference_size,\n                      metrics_logger=None,\n                      ppo_batch_size=100_000,  # batch size - much higher than 300K doesn\'t seem to help most people\n                      policy_layer_sizes=[2048, 2048, 1024, 1024],  # policy network\n                      critic_layer_sizes=[2048, 2048, 1024, 1024],  # critic network\n                      ts_per_iteration=100_000,  # timesteps per training iteration - set this equal to the batch size\n                      exp_buffer_size=300_000,  # size of experience buffer - keep this 2 - 3x the batch size\n                      ppo_minibatch_size=100_000,  # minibatch size - set this as high as your GPU can handle\n                      ppo_ent_coef=0.01,  # entropy coefficient - this determines the impact of exploration\n                      policy_lr=1e-4,  # policy learning rate\n                      critic_lr=1e-4,  # critic learning rate\n                      ppo_epochs=2,   # number of PPO epochs\n                      standardize_returns=True,\n                      standardize_obs=False,\n                      save_every_ts=1_000_000,  # save every 1M steps\n                      timestep_limit=1_000_000_000,  # Train for 1B steps\n                      log_to_wandb=True)\n    learner.learn()\n')),(0,r.kt)("h2",{id:"understanding-the-training-process"},"Understanding the Training Process"),(0,r.kt)("p",null,"With PPO, training happens in cycles. First, the agent collects experience in the game and then PPO uses that experience to improve the agent. During this phase the bot plays games in RocketSim, trying different actions to gather experience. Each time the bot takes an action the game advances 8 physics ticks, and the environment returns a reward and observation. We call this a ",(0,r.kt)("em",{parentName:"p"},"timestep"),". After collecting a certain number of timesteps PPO uses all the collected experience to improve the agent's neural network, making it more likely to repeat actions that led to high rewards and less likely to repeat actions that led to low rewards. We call one of these training cycles an ",(0,r.kt)("em",{parentName:"p"},"iteration"),"."),(0,r.kt)("p",null,"After each iteration, you will see some information displayed in the console about what happened since the last iteration. These metrics include important information about the training process, such as:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Policy Reward"),": Average reward per episode (higher is better)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Policy Entropy"),": How much the policy is exploring (this should settle around a positive number like 2)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Collected Steps Per Second"),": How many timesteps are being collected per second (higher is better)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Consumed Steps Per Second"),": How many timesteps are being consumed per second (higher is better)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Value Loss"),": How well the bot predicts future rewards (lower is better)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Total Steps"),": How many timesteps the bot has collected")),(0,r.kt)("h2",{id:"monitoring-progress"},"Monitoring Progress"),(0,r.kt)("p",null,"RLGym-PPO has integrated support for ",(0,r.kt)("a",{parentName:"p",href:"https://wandb.ai"},"Weights & Biases")," (wandb) for tracking training metrics. Once you set up an account with wandb, set the ",(0,r.kt)("inlineCode",{parentName:"p"},"log_to_wandb")," parameter to ",(0,r.kt)("inlineCode",{parentName:"p"},"True")," in the ",(0,r.kt)("inlineCode",{parentName:"p"},"Learner")," constructor. Then you can view your training progress in the web interface. You'll see graphs of rewards, losses, and other statistics that help you understand how your bot is improving."))}u.isMDXComponent=!0}}]);