"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[793],{4137:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>d});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=r.createContext({}),c=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},u=function(e){var t=c(e.components);return r.createElement(s.Provider,{value:t},e.children)},m="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},_=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),m=c(n),_=i,d=m["".concat(s,".").concat(_)]||m[_]||p[_]||o;return n?r.createElement(d,a(a({ref:t},u),{},{components:n})):r.createElement(d,a({ref:t},u))}));function d(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,a=new Array(o);a[0]=_;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[m]="string"==typeof e?e:i,a[1]=l;for(var c=2;c<o;c++)a[c]=n[c];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}_.displayName="MDXCreateElement"},7704:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var r=n(7462),i=(n(7294),n(4137));const o={title:"Quick Start Guide",sidebar_position:3},a="Quick Start Guide",l={unversionedId:"Getting Started/quickstart",id:"Getting Started/quickstart",title:"Quick Start Guide",description:"In this guide we'll install RLGym with RocketSim and train a simple Rocket League bot with an algorithm from RLGym Learn. To train a bot, we will first set up an environment where it can play, then create a learner and start training. In this guide we'll use all the default settings, but you can experiment with different values to see how they affect your bot's performance.",source:"@site/docs/Getting Started/quickstart.md",sourceDirName:"Getting Started",slug:"/Getting Started/quickstart",permalink:"/Getting Started/quickstart",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Quick Start Guide",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Overview",permalink:"/Getting Started/overview"},next:{title:"Training an Agent",permalink:"/Rocket League/training_an_agent"}},s={},c=[{value:"Installation",id:"installation",level:2},{value:"Training Your First Agent",id:"training-your-first-agent",level:2}],u={toc:c},m="wrapper";function p(e){let{components:t,...n}=e;return(0,i.kt)(m,(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"quick-start-guide"},"Quick Start Guide"),(0,i.kt)("p",null,"In this guide we'll install RLGym with RocketSim and train a simple Rocket League bot with an algorithm from RLGym Learn. To train a bot, we will first set up an environment where it can play, then create a learner and start training. In this guide we'll use all the default settings, but you can experiment with different values to see how they affect your bot's performance. "),(0,i.kt)("h2",{id:"installation"},"Installation"),(0,i.kt)("p",null,"Let's start by installing RLGym with RocketSim support. RocketSim is a headless simulator for Rocket League, which means your bot can train much faster than in the real game. We'll also install RLGym-PPO, which provides an implementation of PPO compatible with RLGym."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install rlgym[rl-sim]\npip install git+https://github.com/AechPro/rlgym-ppo\n")),(0,i.kt)("p",null,"If you have an NVIDIA GPU, you'll also want to install PyTorch with CUDA support from ",(0,i.kt)("a",{parentName:"p",href:"https://pytorch.org"},"pytorch.org"),"."),(0,i.kt)("h2",{id:"training-your-first-agent"},"Training Your First Agent"),(0,i.kt)("p",null,"Next, we'll create a Rocket League environment where our bot can train. We'll set up a 2v2 match with the typical kickoff positions, let the agent see all the relevant information from the game, and reward it for scoring goals and touching the ball. While the game will end as usual after a 5 minute timer, we're also going to add a timeout condition to end the episode if the agent doesn't touch the ball for 30 seconds. We do this because we don't want the bot to waste a bunch of time flopping around without touching the ball."),(0,i.kt)("p",null,"The following code will set up our simple 2v2 environment and start training our bot with the Proximal Policy Optimization (PPO) algorithm implemented in RLGym Learn."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def build_rlgym_v2_env():\n    from rlgym.api import RLGym\n    from rlgym.rocket_league.action_parsers import LookupTableAction, RepeatAction\n    from rlgym.rocket_league.done_conditions import GoalCondition, NoTouchTimeoutCondition, TimeoutCondition\n    from rlgym.rocket_league.obs_builders import DefaultObs\n    from rlgym.rocket_league.reward_functions import CombinedReward, GoalReward, TouchReward\n    from rlgym.rocket_league.sim import RocketSimEngine\n    from rlgym.rocket_league.state_mutators import MutatorSequence, FixedTeamSizeMutator, KickoffMutator\n    from rlgym.rocket_league import common_values\n    from rlgym_ppo.util import RLGymV2GymWrapper\n    import numpy as np\n\n    spawn_opponents = True\n    team_size = 1\n    blue_team_size = team_size\n    orange_team_size = team_size if spawn_opponents else 0\n    action_repeat = 8\n    no_touch_timeout_seconds = 10\n    game_timeout_seconds = 300\n\n    action_parser = RepeatAction(LookupTableAction(), repeats=action_repeat)\n    termination_condition = GoalCondition()\n    truncation_condition = AnyCondition(NoTouchTimeoutCondition(timeout_seconds=no_touch_timeout_seconds), TimeoutCondition(timeout_seconds=game_timeout_seconds))\n\n    reward_fn = CombinedReward((GoalReward(), 10), (TouchReward(), 0.1))\n\n    obs_builder = DefaultObs(zero_padding=None,\n                             pos_coef=np.asarray([1 / common_values.SIDE_WALL_X, 1 / common_values.BACK_NET_Y, 1 / common_values.CEILING_Z]),\n                             ang_coef=1 / np.pi,\n                             lin_vel_coef=1 / common_values.CAR_MAX_SPEED,\n                             ang_vel_coef=1 / common_values.CAR_MAX_ANG_VEL,\n                             boost_coef=1 / 100.0,)\n\n    state_mutator = MutatorSequence(FixedTeamSizeMutator(blue_size=blue_team_size, orange_size=orange_team_size),\n                                    KickoffMutator())\n    rlgym_env = RLGym(\n        state_mutator=state_mutator,\n        obs_builder=obs_builder,\n        action_parser=action_parser,\n        reward_fn=reward_fn,\n        termination_cond=termination_condition,\n        truncation_cond=truncation_condition,\n        transition_engine=RocketSimEngine())\n\n    return RLGymV2GymWrapper(rlgym_env)\n\n\nif __name__ == "__main__":\n    from rlgym_ppo import Learner\n\n    # 32 processes\n    n_proc = 32\n\n    # educated guess - could be slightly higher or lower\n    min_inference_size = max(1, int(round(n_proc * 0.9)))\n\n    learner = Learner(build_rlgym_v2_env,\n                      n_proc=n_proc,\n                      min_inference_size=min_inference_size,\n                      metrics_logger=None,\n                      ppo_batch_size=50000, # batch size - set this number to as large as your GPU can handle\n                      policy_layer_sizes=[2048, 2048, 1024, 1024], # policy network\n                      critic_layer_sizes=[2048, 2048, 1024, 1024], # value network\n                      ts_per_iteration=50000, # timesteps per training iteration - set this equal to the batch size\n                      exp_buffer_size=150000, # size of experience buffer - keep this 2 - 3x the batch size\n                      ppo_minibatch_size=50000, # minibatch size - set this less than or equal to the batch size\n                      ppo_ent_coef=0.01, # entropy coefficient - this determines the impact of exploration on the policy\n                      policy_lr=5e-5, # policy learning rate\n                      critic_lr=5e-5, # value function learning rate\n                      ppo_epochs=1,   # number of PPO epochs\n                      standardize_returns=True,\n                      standardize_obs=False,\n                      save_every_ts=1_000_000, # save every 1M steps\n                      timestep_limit=1_000_000_000, # Train for 1B steps\n                      log_to_wandb=True)\n    learner.learn()\n')),(0,i.kt)("p",null,"This code will train a 2v2 bot for 1 billion timesteps (That's over 18,500 hours of in-game time!), saving checkpoints every 1 million timesteps. You can stop the training process at any time by pressing P while the program is in focus."),(0,i.kt)("p",null,"For a more complete guide for training your first agent, refer to our Rocket League tutorial on ",(0,i.kt)("a",{parentName:"p",href:"../Rocket%20League/training_an_agent"},"training an agent"),"."))}p.isMDXComponent=!0}}]);