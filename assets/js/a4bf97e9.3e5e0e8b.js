"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[187],{4137:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>m});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=r.createContext({}),l=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=l(n),f=a,m=p["".concat(c,".").concat(f)]||p[f]||d[f]||o;return n?r.createElement(m,i(i({ref:t},u),{},{components:n})):r.createElement(m,i({ref:t},u))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=f;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[p]="string"==typeof e?e:a,i[1]=s;for(var l=2;l<o;l++)i[l]=n[l];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}f.displayName="MDXCreateElement"},645:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var r=n(7462),a=(n(7294),n(4137));const o={title:"Action Parsers"},i="Action Parsers",s={unversionedId:"Rocket League/Configuration Objects/action_parsers",id:"Rocket League/Configuration Objects/action_parsers",title:"Action Parsers",description:"Action parsers are how your agent's decisions get turned into actual game inputs. They take whatever your policy outputs and convert it into the 8 controller inputs that RocketSim and Rocket League understand (things like throttle, steering, etc.). Check out our Game Values cheatsheet to see what these inputs are.",source:"@site/docs/Rocket League/Configuration Objects/action_parsers.md",sourceDirName:"Rocket League/Configuration Objects",slug:"/Rocket League/Configuration Objects/action_parsers",permalink:"/Rocket League/Configuration Objects/action_parsers",draft:!1,tags:[],version:"current",frontMatter:{title:"Action Parsers"},sidebar:"tutorialSidebar",previous:{title:"Training an Agent",permalink:"/Rocket League/training_an_agent"},next:{title:"Done Conditions",permalink:"/Rocket League/Configuration Objects/done_conditions"}},c={},l=[{value:"How They Work",id:"how-they-work",level:2},{value:"Creating Your Own",id:"creating-your-own",level:2}],u={toc:l},p="wrapper";function d(e){let{components:t,...n}=e;return(0,a.kt)(p,(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"action-parsers"},"Action Parsers"),(0,a.kt)("p",null,"Action parsers are how your agent's decisions get turned into actual game inputs. They take whatever your policy outputs and convert it into the 8 controller inputs that RocketSim and Rocket League understand (things like throttle, steering, etc.). Check out our ",(0,a.kt)("a",{parentName:"p",href:"../../Cheatsheets/game_values"},"Game Values")," cheatsheet to see what these inputs are."),(0,a.kt)("h2",{id:"how-they-work"},"How They Work"),(0,a.kt)("p",null,"Every ",(0,a.kt)("inlineCode",{parentName:"p"},"ActionParser")," needs three methods:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# Called during the initialization of the environment, this is used to inform the learning algorithm how many outputs\n# the policy must provide, and their type.\ndef get_action_space(self, agent: AgentID) -> SpaceType:\n\n# Called every time `TransitionEngine.create_base_state()` is called.\ndef reset(self, initial_state: StateType, shared_info: Dict[str, Any]) -> None:\n    \n# Called every time `TransitionEngine.step()` is called. This function is responsible for translating actions into transition engine inputs for each agent.\ndef parse_actions(self, actions: Dict[AgentID, ActionType], state: StateType, shared_info: Dict[str, Any]) -> Dict[AgentID, EngineActionType]:\n")),(0,a.kt)("h2",{id:"creating-your-own"},"Creating Your Own"),(0,a.kt)("p",null,"To create a custom action parser, inherit from ",(0,a.kt)("inlineCode",{parentName:"p"},"ActionParser")," class and implement those methods. Here's an example that takes 8 continuous values between -1 and 1, and converts the last 3 into binary (0 or 1) inputs to match what the game expects:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from typing import Dict, Any\n\nimport numpy as np\n\nfrom rlgym.api import ActionParser, AgentID\nfrom rlgym.rocket_league.api import GameState\n\nclass ContinuousAction(ActionParser[AgentID, np.ndarray, np.ndarray, GameState, int]):\n    """\n    Simple continuous action space that maps an array of 8 values on the interval [-1, 1] into an array of valid car\n    controls.\n    """\n\n    def __init__(self):\n        super().__init__()\n        # Rocket League expects 8 values per controller input.\n        self._n_controller_inputs = 8\n        \n    def get_action_space(self) -> tuple:\n        return float(self._n_controller_inputs), \'continuous\'\n\n    def reset(self, initial_state: GameState, shared_info: Dict[str, Any]) -> None:\n        pass\n\n    def parse_actions(self, actions: Dict[AgentID, np.ndarray], state: GameState, shared_info: Dict[str, Any]) -> Dict[AgentID, np.ndarray]:\n        parsed_actions = {}\n        \n        # Loop over the agent action dictionary\n        for agent, action in actions.items():\n            # Copy the action into a new array\n            car_controls = np.zeros(self._n_controller_inputs)\n            car_controls[:] = action[:]\n            \n            # All the actions from our policy will be on the interval [-1, 1], but the last 3 values in the car controls\n            # need to be either 0 or 1. We will shift and round the result such that any value below 0 becomes 0 and\n            # any value above 0 becomes 1.\n            car_controls[-3:] = np.round((car_controls[-3:] + 1) / 2)\n            parsed_actions[agent] = car_controls\n\n        return parsed_actions\n')),(0,a.kt)("p",null,"Now we can pass an instance of our ",(0,a.kt)("inlineCode",{parentName:"p"},"ContinuousAction")," to RLGym whenever we make an environment!"))}d.isMDXComponent=!0}}]);