"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[793],{4137:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>g});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=r.createContext({}),u=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},c=function(e){var t=u(e.components);return r.createElement(l.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},_=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=u(n),_=i,g=p["".concat(l,".").concat(_)]||p[_]||m[_]||o;return n?r.createElement(g,a(a({ref:t},c),{},{components:n})):r.createElement(g,a({ref:t},c))}));function g(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,a=new Array(o);a[0]=_;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:i,a[1]=s;for(var u=2;u<o;u++)a[u]=n[u];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}_.displayName="MDXCreateElement"},7704:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>u});var r=n(7462),i=(n(7294),n(4137));const o={title:"Quick Start Guide",sidebar_position:3},a="Quick Start Guide",s={unversionedId:"Getting Started/quickstart",id:"Getting Started/quickstart",title:"Quick Start Guide",description:"Let's get you started with RLGym by installing it with RocketSim and training a basic Rocket League agent using RLGym-PPO. We'll set up a training environment, configure a learner, and get training. We'll use default settings here, but you can adjust these later to tune how your agent learns.",source:"@site/docs/Getting Started/quickstart.md",sourceDirName:"Getting Started",slug:"/Getting Started/quickstart",permalink:"/Getting Started/quickstart",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Quick Start Guide",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Overview",permalink:"/Getting Started/overview"},next:{title:"Training an Agent",permalink:"/Rocket League/training_an_agent"}},l={},u=[{value:"Installation",id:"installation",level:2},{value:"Training Your First Agent",id:"training-your-first-agent",level:2}],c={toc:u},p="wrapper";function m(e){let{components:t,...n}=e;return(0,i.kt)(p,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"quick-start-guide"},"Quick Start Guide"),(0,i.kt)("p",null,"Let's get you started with RLGym by installing it with RocketSim and training a basic Rocket League agent using RLGym-PPO. We'll set up a training environment, configure a learner, and get training. We'll use default settings here, but you can adjust these later to tune how your agent learns."),(0,i.kt)("h2",{id:"installation"},"Installation"),(0,i.kt)("p",null,"First, let's get RLGym with RocketSim support and install RLGym-PPO:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install rlgym[rl-sim]\npip install git+https://github.com/AechPro/rlgym-ppo\n")),(0,i.kt)("p",null,"If you have an NVIDIA GPU, grab PyTorch with CUDA support from ",(0,i.kt)("a",{parentName:"p",href:"https://pytorch.org"},"pytorch.org")," to speed up training."),(0,i.kt)("h2",{id:"training-your-first-agent"},"Training Your First Agent"),(0,i.kt)("p",null,"Now we'll set up a 2v2 rocket league environment with standard kickoff positions. The environment will tell the agent what's happening in the game and reward it for scoring goals and touching the ball. While normal matches last 5 minutes, we'll add a 30-second timeout if the agent doesn't touch the ball - this helps avoid wasting time on unproductive training episodes."),(0,i.kt)("p",null,"The code below configures the 2v2 environment and initializes training using Proximal Policy Optimization (PPO):"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def build_rlgym_v2_env():\n    from rlgym.api import RLGym\n    from rlgym.rocket_league.action_parsers import LookupTableAction, RepeatAction\n    from rlgym.rocket_league.done_conditions import GoalCondition, NoTouchTimeoutCondition, TimeoutCondition, AnyCondition\n    from rlgym.rocket_league.obs_builders import DefaultObs\n    from rlgym.rocket_league.reward_functions import CombinedReward, GoalReward, TouchReward\n    from rlgym.rocket_league.sim import RocketSimEngine\n    from rlgym.rocket_league.state_mutators import MutatorSequence, FixedTeamSizeMutator, KickoffMutator\n    from rlgym.rocket_league import common_values\n    from rlgym_ppo.util import RLGymV2GymWrapper\n    import numpy as np\n\n    spawn_opponents = True\n    team_size = 2\n    blue_team_size = team_size\n    orange_team_size = team_size if spawn_opponents else 0\n    action_repeat = 8\n    no_touch_timeout_seconds = 30\n    game_timeout_seconds = 300\n\n    action_parser = RepeatAction(LookupTableAction(), repeats=action_repeat)\n    termination_condition = GoalCondition()\n    truncation_condition = AnyCondition(NoTouchTimeoutCondition(timeout_seconds=no_touch_timeout_seconds), TimeoutCondition(timeout_seconds=game_timeout_seconds))\n\n    reward_fn = CombinedReward((GoalReward(), 10), (TouchReward(), 0.1))\n\n    obs_builder = DefaultObs(zero_padding=None,\n                             pos_coef=np.asarray([1 / common_values.SIDE_WALL_X, 1 / common_values.BACK_NET_Y, 1 / common_values.CEILING_Z]),\n                             ang_coef=1 / np.pi,\n                             lin_vel_coef=1 / common_values.CAR_MAX_SPEED,\n                             ang_vel_coef=1 / common_values.CAR_MAX_ANG_VEL,\n                             boost_coef=1 / 100.0,)\n\n    state_mutator = MutatorSequence(FixedTeamSizeMutator(blue_size=blue_team_size, orange_size=orange_team_size),\n                                    KickoffMutator())\n    rlgym_env = RLGym(\n        state_mutator=state_mutator,\n        obs_builder=obs_builder,\n        action_parser=action_parser,\n        reward_fn=reward_fn,\n        termination_cond=termination_condition,\n        truncation_cond=truncation_condition,\n        transition_engine=RocketSimEngine())\n\n    return RLGymV2GymWrapper(rlgym_env)\n\n\nif __name__ == "__main__":\n    from rlgym_ppo import Learner\n\n    # 8 processes\n    n_proc = 8\n\n    # educated guess - could be slightly higher or lower\n    min_inference_size = max(1, int(round(n_proc * 0.9)))\n\n    learner = Learner(build_rlgym_v2_env,\n                      n_proc=n_proc,\n                      min_inference_size=min_inference_size,\n                      metrics_logger=None,\n                      ppo_batch_size=50000, # batch size - set this number to as large as your GPU can handle\n                      policy_layer_sizes=[512, 512], # policy network\n                      critic_layer_sizes=[512, 512], # value network\n                      ts_per_iteration=50000, # timesteps per training iteration - set this equal to the batch size\n                      exp_buffer_size=150000, # size of experience buffer - keep this 2 - 3x the batch size\n                      ppo_minibatch_size=50000, # minibatch size - set this less than or equal to the batch size\n                      ppo_ent_coef=0.01, # entropy coefficient - this determines the impact of exploration on the policy\n                      policy_lr=5e-5, # policy learning rate\n                      critic_lr=5e-5, # value function learning rate\n                      ppo_epochs=1,   # number of PPO epochs\n                      standardize_returns=True,\n                      standardize_obs=False,\n                      save_every_ts=1_000_000, # save every 1M steps\n                      timestep_limit=1_000_000_000, # Train for 1B steps\n                      log_to_wandb=True)\n    learner.learn()\n')),(0,i.kt)("p",null,"For a more complete guide to training your first agent, refer to our Rocket League tutorial on ",(0,i.kt)("a",{parentName:"p",href:"../Rocket%20League/training_an_agent"},"training an agent"),"."))}m.isMDXComponent=!0}}]);