"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[359],{4137:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>u});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=r.createContext({}),d=function(e){var n=r.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},c=function(e){var n=d(e.components);return r.createElement(l.Provider,{value:n},e.children)},p="mdxType",f={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},m=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(t),m=a,u=p["".concat(l,".").concat(m)]||p[m]||f[m]||i;return t?r.createElement(u,o(o({ref:n},c),{},{components:t})):r.createElement(u,o({ref:n},c))}));function u(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[p]="string"==typeof e?e:a,o[1]=s;for(var d=2;d<i;d++)o[d]=t[d];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}m.displayName="MDXCreateElement"},1355:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>f,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var r=t(7462),a=(t(7294),t(4137));const i={title:"Creating an Environment",sidebar_position:2},o="Creating an Environment",s={unversionedId:"Custom Environments/custom-environment",id:"Custom Environments/custom-environment",title:"Creating an Environment",description:"This tutorial demonstrates how to implement a grid world environment using the RLGym API. Each RLGym environment requires implementing the configuration objects described in the RLGym overview. The following example illustrates an implementation of each required component.",source:"@site/docs/Custom Environments/custom-environment.md",sourceDirName:"Custom Environments",slug:"/Custom Environments/custom-environment",permalink:"/Custom Environments/custom-environment",draft:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Creating an Environment",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"State Mutators",permalink:"/Rocket League/Configuration Objects/state_mutators"},next:{title:"List of Game Values",permalink:"/Cheatsheets/game_values"}},l={},d=[{value:"Grid World Example",id:"grid-world-example",level:2}],c={toc:d},p="wrapper";function f(e){let{components:n,...t}=e;return(0,a.kt)(p,(0,r.Z)({},c,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"creating-an-environment"},"Creating an Environment"),(0,a.kt)("p",null,"This tutorial demonstrates how to implement a grid world environment using the RLGym API. Each RLGym environment requires implementing the configuration objects described in the ",(0,a.kt)("a",{parentName:"p",href:"/Getting%20Started/overview"},"RLGym overview"),". The following example illustrates an implementation of each required component."),(0,a.kt)("h2",{id:"grid-world-example"},"Grid World Example"),(0,a.kt)("p",null,"We begin by defining the state of our environment, and a transition engine that handles the environment dynamics."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from typing import Dict, List, Tuple, Optional\nimport numpy as np\nfrom dataclasses import dataclass\nfrom rlgym.api import TransitionEngine, StateMutator, ObsBuilder, ActionParser, RewardFunction, DoneCondition\n\n# First, define our state type\n@dataclass\nclass GridWorldState:\n    agent_pos: np.ndarray  # [x, y]\n    target_pos: np.ndarray  # [x, y]\n    obstacles: List[np.ndarray]  # List of [x, y] positions\n    grid_size: int\n    steps: int = 0\n\n# Now we implement our Transition Engine, which is the core of the environment.\nclass GridWorldEngine(TransitionEngine[int, GridWorldState, int]):\n    """Handles the core game logic"""\n    def __init__(self, grid_size: int):\n        self.grid_size = grid_size\n        self._state = None\n        self._config = {}\n        \n    @property\n    def agents(self) -> List[int]:\n        return [0]  # Single agent environment\n        \n    @property\n    def max_num_agents(self) -> int:\n        return 1  # This environment only supports one agent\n        \n    @property\n    def state(self) -> GridWorldState:\n        return self._state\n        \n    @property\n    def config(self) -> Dict[str, Any]:\n        return self._config\n        \n    @config.setter\n    def config(self, value: Dict[str, Any]):\n        self._config = value\n        \n    def step(self, actions: Dict[int, int], shared_info: Dict[str, Any]) -> GridWorldState:\n        action = actions[0]  # Get action for our single agent\n        current_pos = self._state.agent_pos.copy()\n        \n        # Apply movement: 0=up, 1=right, 2=down, 3=left\n        if action == 0:   current_pos[1] += 1\n        elif action == 1: current_pos[0] += 1\n        elif action == 2: current_pos[1] -= 1\n        elif action == 3: current_pos[0] -= 1\n        \n        # Ensure we stay in bounds\n        current_pos = np.clip(current_pos, 0, self.grid_size - 1)\n        \n        # Check if move is valid (not into obstacle)\n        if not any(np.array_equal(current_pos, obs) for obs in self._state.obstacles):\n            self._state.agent_pos = current_pos\n            \n        self._state.steps += 1\n        return self._state\n        \n    def create_base_state(self) -> GridWorldState:\n        # Create a minimal state for the mutator to modify\n        return GridWorldState(\n            agent_pos=np.zeros(2),  # Will be set by mutator\n            target_pos=np.zeros(2),  # Will be set by mutator\n            obstacles=[],  # Will be set by mutator\n            grid_size=self.grid_size,\n            steps=0\n        )\n        \n    def reset(self, initial_state: Optional[GridWorldState] = None) -> None:\n        """Reset the engine with an optional initial state"""\n        self._state = initial_state if initial_state is not None else self.create_base_state()\n')),(0,a.kt)("p",null,"Now we implement the remaining configuration objects for our environment."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# We need to define a state mutator, which is responsible for modifying the environment state.\nclass GridWorldMutator(StateMutator[GridWorldState]):\n    """Controls environment reset and state modifications"""\n    def __init__(self, grid_size: int, num_obstacles: int = 3):\n        self.grid_size = grid_size\n        self.num_obstacles = num_obstacles\n        \n    def apply(self, state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        # Random agent and target positions\n        state.agent_pos = np.random.randint(0, self.grid_size, size=2)\n        state.target_pos = np.random.randint(0, self.grid_size, size=2)\n        \n        # Random obstacle positions (ensuring they don\'t overlap)\n        state.obstacles = []\n        while len(state.obstacles) < self.num_obstacles:\n            obs = np.random.randint(0, self.grid_size, size=2)\n            if not (np.array_equal(obs, state.agent_pos) or \n                   np.array_equal(obs, state.target_pos) or \n                   any(np.array_equal(obs, o) for o in state.obstacles)):\n                state.obstacles.append(obs)\n\n# Here is the Observation Builder, which will convert the environment state into agent observations.\nclass GridWorldObs(ObsBuilder[int, np.ndarray, GridWorldState, np.ndarray]):\n    """Converts state into agent observations"""\n        \n    def get_obs_space(self, agent: int) -> np.ndarray:\n        # [agent_x, agent_y, target_x, target_y, obstacles]\n        return np.zeros(4 + 2*3, dtype=np.float32)  # Assuming max 3 obstacles\n        \n    def reset(self, agents: List[int], initial_state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        pass\n        \n    def build_obs(self, agents: List[int], state: GridWorldState, shared_info: Dict[str, Any]) -> Dict[int, np.ndarray]:\n        # Build observation for each agent\n        observations = {}\n        for agent in agents:\n            # [agent_x, agent_y, target_x, target_y, obstacle positions]\n            obs = np.concatenate([\n                state.agent_pos,\n                state.target_pos,\n                np.concatenate(state.obstacles)\n            ])\n            observations[agent] = obs\n        return observations\n\n# Next we need an Action Parser, which will define what actions agents can take. \n# Because we are using a simple discrete action space, we don\'t need to do anything special here, \n# so we\'ll just pass the actions from the agent straight through to the transition engine.\nclass GridWorldActions(ActionParser[int, int, int, GridWorldState, int]):\n    """Defines the action space and parsing"""\n    def get_action_space(self, agent: int) -> int:\n        return 4  # Up, Right, Down, Left\n        \n    def reset(self, agents: List[int], initial_state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        pass  # No state to reset\n        \n    def parse_actions(self, actions: Dict[int, int], state: GridWorldState, shared_info: Dict[str, Any]) -> Dict[int, int]:\n        # Actions are already in the correct format\n        return actions\n\n# Now we need a Reward Function, which will calculate rewards for the agents.\nclass GridWorldReward(RewardFunction[int, GridWorldState, float]):\n    """Calculates rewards for actions"""\n    def __init__(self, goal_reward: float = 10.0, step_penalty: float = -0.1):\n        self.goal_reward = goal_reward\n        self.step_penalty = step_penalty\n        \n    def reset(self, agents: List[int], initial_state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        pass  # No state to reset\n        \n    def get_rewards(self, agents: List[int], state: GridWorldState, \n                   is_terminated: Dict[int, bool], is_truncated: Dict[int, bool],\n                   shared_info: Dict[str, Any]) -> Dict[int, float]:\n        rewards = {}\n        for agent in agents:\n            if np.array_equal(state.agent_pos, state.target_pos):\n                # If we reached the target, provide the goal reward.\n                rewards[agent] = self.goal_reward\n            else:\n                # If we haven\'t reached the target, apply a step penalty.\n                rewards[agent] = self.step_penalty\n        return rewards\n\n# Finally we\'ll create Terminal and Truncated conditions, which will determine when episodes end naturally.\nclass GridWorldTerminalCondition(DoneCondition[int, GridWorldState]):\n    """Determines when episodes naturally end (reaching the goal)"""\n    def reset(self, agents: List[int], initial_state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        pass\n        \n    def is_done(self, agent: int, state: GridWorldState) -> bool:\n        # Episode ends naturally when we reach the target\n        return np.array_equal(state.agent_pos, state.target_pos)\n\nclass GridWorldTruncatedCondition(DoneCondition[int, GridWorldState]):\n    """Determines when episodes are cut short (timeout)"""\n    def __init__(self, max_steps: int = 100):\n        self.max_steps = max_steps\n        \n    def reset(self, agents: List[int], initial_state: GridWorldState, shared_info: Dict[str, Any]) -> None:\n        pass\n        \n    def is_done(self, agent: int, state: GridWorldState) -> bool:\n        # Episode is truncated if we exceed max steps\n        return state.steps >= self.max_steps\n\n')),(0,a.kt)("p",null,"With all configuration objects implemented, we can construct the environment by passing an instance of each object to the RLGym constructor."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# Build the environment\nenv = RLGym(\n        state_mutator=GridWorldMutator(grid_size),\n        obs_builder=GridWorldObs(),\n        action_parser=GridWorldActions(),\n        reward_fn=GridWorldReward(),\n        transition_engine=GridWorldEngine(grid_size),\n        termination_cond=GridWorldTerminalCondition(),\n        truncation_cond=GridWorldTruncatedCondition(),\n    )\n\n# Interact with our gridworld like any other RLGym environment.\nobs = env.reset()\nep_rew = 0\n\nfor _ in range(1000):\n    action = {0: env.action_space.sample()}  # Random action\n    obs, reward, done, truncated, info = env.step(action)\n    ep_rew += reward[0] # Reward for agent 0\n    \n    if done or truncated:\n        obs = env.reset()\n        print(f"Episode reward: {ep_rew}")\n        ep_rew = 0\n')),(0,a.kt)("p",null,"The environment is now ready for integration with a learning algorithm to train a grid world agent."))}f.isMDXComponent=!0}}]);